{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Logistic Regression - Cumulative Lab\n", "\n", "## Introduction\n", "\n", "In this cumulative lab, you will walk through a complete machine learning workflow with logistic regression, including data preparation, modeling (including hyperparameter tuning), and final model evaluation.\n", "\n", "## Objectives\n", "\n", "You will be able to:\n", "\n", "* Practice identifying and applying appropriate preprocessing steps\n", "* Perform an iterative modeling process, starting from a baseline model\n", "* Practice model validation\n", "* Practice choosing a final logistic regression model and evaluating its performance"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Your Task: Complete an End-to-End ML Process with Logistic Regression on the Forest Cover Dataset\n", "\n", "![forest road](images/forest_road.jpg)\n", "\n", "<span>Photo by <a href=\"https://unsplash.com/@von_co?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Ivana Cajina</a> on <a href=\"https://unsplash.com/s/photos/forest-satellite?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></span>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Business and Data Understanding\n", "\n", "Here we will be using an adapted version of the forest cover dataset from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/covertype). Each record represents a 30 x 30 meter cell of land within Roosevelt National Forest in northern Colorado, which has been labeled as `Cover_Type` 1 for \"Cottonwood/Willow\" and `Cover_Type` 0 for \"Ponderosa Pine\". (The original dataset contained 7 cover types but we have simplified it.)\n", "\n", "The task is to predict the `Cover_Type` based on the available cartographic variables:"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>Elevation</th>\n", "      <th>Aspect</th>\n", "      <th>Slope</th>\n", "      <th>Horizontal_Distance_To_Hydrology</th>\n", "      <th>Vertical_Distance_To_Hydrology</th>\n", "      <th>Horizontal_Distance_To_Roadways</th>\n", "      <th>Hillshade_9am</th>\n", "      <th>Hillshade_Noon</th>\n", "      <th>Hillshade_3pm</th>\n", "      <th>Horizontal_Distance_To_Fire_Points</th>\n", "      <th>...</th>\n", "      <th>Soil_Type_31</th>\n", "      <th>Soil_Type_32</th>\n", "      <th>Soil_Type_33</th>\n", "      <th>Soil_Type_34</th>\n", "      <th>Soil_Type_35</th>\n", "      <th>Soil_Type_36</th>\n", "      <th>Soil_Type_37</th>\n", "      <th>Soil_Type_38</th>\n", "      <th>Soil_Type_39</th>\n", "      <th>Cover_Type</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>2553</td>\n", "      <td>235</td>\n", "      <td>17</td>\n", "      <td>351</td>\n", "      <td>95</td>\n", "      <td>780</td>\n", "      <td>188</td>\n", "      <td>253</td>\n", "      <td>199</td>\n", "      <td>1410</td>\n", "      <td>...</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>2011</td>\n", "      <td>344</td>\n", "      <td>17</td>\n", "      <td>313</td>\n", "      <td>29</td>\n", "      <td>404</td>\n", "      <td>183</td>\n", "      <td>211</td>\n", "      <td>164</td>\n", "      <td>300</td>\n", "      <td>...</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>2022</td>\n", "      <td>24</td>\n", "      <td>13</td>\n", "      <td>391</td>\n", "      <td>42</td>\n", "      <td>509</td>\n", "      <td>212</td>\n", "      <td>212</td>\n", "      <td>134</td>\n", "      <td>421</td>\n", "      <td>...</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>2038</td>\n", "      <td>50</td>\n", "      <td>17</td>\n", "      <td>408</td>\n", "      <td>71</td>\n", "      <td>474</td>\n", "      <td>226</td>\n", "      <td>200</td>\n", "      <td>102</td>\n", "      <td>283</td>\n", "      <td>...</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>2018</td>\n", "      <td>341</td>\n", "      <td>27</td>\n", "      <td>351</td>\n", "      <td>34</td>\n", "      <td>390</td>\n", "      <td>152</td>\n", "      <td>188</td>\n", "      <td>168</td>\n", "      <td>190</td>\n", "      <td>...</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>...</th>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>38496</th>\n", "      <td>2396</td>\n", "      <td>153</td>\n", "      <td>20</td>\n", "      <td>85</td>\n", "      <td>17</td>\n", "      <td>108</td>\n", "      <td>240</td>\n", "      <td>237</td>\n", "      <td>118</td>\n", "      <td>837</td>\n", "      <td>...</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>38497</th>\n", "      <td>2391</td>\n", "      <td>152</td>\n", "      <td>19</td>\n", "      <td>67</td>\n", "      <td>12</td>\n", "      <td>95</td>\n", "      <td>240</td>\n", "      <td>237</td>\n", "      <td>119</td>\n", "      <td>845</td>\n", "      <td>...</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>38498</th>\n", "      <td>2386</td>\n", "      <td>159</td>\n", "      <td>17</td>\n", "      <td>60</td>\n", "      <td>7</td>\n", "      <td>90</td>\n", "      <td>236</td>\n", "      <td>241</td>\n", "      <td>130</td>\n", "      <td>854</td>\n", "      <td>...</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>38499</th>\n", "      <td>2384</td>\n", "      <td>170</td>\n", "      <td>15</td>\n", "      <td>60</td>\n", "      <td>5</td>\n", "      <td>90</td>\n", "      <td>230</td>\n", "      <td>245</td>\n", "      <td>143</td>\n", "      <td>864</td>\n", "      <td>...</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>38500</th>\n", "      <td>2383</td>\n", "      <td>165</td>\n", "      <td>13</td>\n", "      <td>60</td>\n", "      <td>4</td>\n", "      <td>67</td>\n", "      <td>231</td>\n", "      <td>244</td>\n", "      <td>141</td>\n", "      <td>875</td>\n", "      <td>...</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "      <td>0</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "<p>38501 rows \u00d7 55 columns</p>\n", "</div>"], "text/plain": ["       Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n", "0           2553     235     17                               351   \n", "1           2011     344     17                               313   \n", "2           2022      24     13                               391   \n", "3           2038      50     17                               408   \n", "4           2018     341     27                               351   \n", "...          ...     ...    ...                               ...   \n", "38496       2396     153     20                                85   \n", "38497       2391     152     19                                67   \n", "38498       2386     159     17                                60   \n", "38499       2384     170     15                                60   \n", "38500       2383     165     13                                60   \n", "\n", "       Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n", "0                                  95                              780   \n", "1                                  29                              404   \n", "2                                  42                              509   \n", "3                                  71                              474   \n", "4                                  34                              390   \n", "...                               ...                              ...   \n", "38496                              17                              108   \n", "38497                              12                               95   \n", "38498                               7                               90   \n", "38499                               5                               90   \n", "38500                               4                               67   \n", "\n", "       Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n", "0                188             253            199   \n", "1                183             211            164   \n", "2                212             212            134   \n", "3                226             200            102   \n", "4                152             188            168   \n", "...              ...             ...            ...   \n", "38496            240             237            118   \n", "38497            240             237            119   \n", "38498            236             241            130   \n", "38499            230             245            143   \n", "38500            231             244            141   \n", "\n", "       Horizontal_Distance_To_Fire_Points  ...  Soil_Type_31  Soil_Type_32  \\\n", "0                                    1410  ...             0             0   \n", "1                                     300  ...             0             0   \n", "2                                     421  ...             0             0   \n", "3                                     283  ...             0             0   \n", "4                                     190  ...             0             0   \n", "...                                   ...  ...           ...           ...   \n", "38496                                 837  ...             0             0   \n", "38497                                 845  ...             0             0   \n", "38498                                 854  ...             0             0   \n", "38499                                 864  ...             0             0   \n", "38500                                 875  ...             0             0   \n", "\n", "       Soil_Type_33  Soil_Type_34  Soil_Type_35  Soil_Type_36  Soil_Type_37  \\\n", "0                 0             0             0             0             0   \n", "1                 0             0             0             0             0   \n", "2                 0             0             0             0             0   \n", "3                 0             0             0             0             0   \n", "4                 0             0             0             0             0   \n", "...             ...           ...           ...           ...           ...   \n", "38496             0             0             0             0             0   \n", "38497             0             0             0             0             0   \n", "38498             0             0             0             0             0   \n", "38499             0             0             0             0             0   \n", "38500             0             0             0             0             0   \n", "\n", "       Soil_Type_38  Soil_Type_39  Cover_Type  \n", "0                 0             0           0  \n", "1                 0             0           0  \n", "2                 0             0           0  \n", "3                 0             0           0  \n", "4                 0             0           0  \n", "...             ...           ...         ...  \n", "38496             0             0           0  \n", "38497             0             0           0  \n", "38498             0             0           0  \n", "38499             0             0           0  \n", "38500             0             0           0  \n", "\n", "[38501 rows x 55 columns]"]}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": ["import pandas as pd\n", "\n", "df = pd.read_csv('data/forest_cover.csv')\n", "df"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As you can see, we have over 38,000 rows, each with 54 feature columns and 1 target column:\n", "\n", "* `Elevation`: Elevation in meters\n", "* `Aspect`: Aspect in degrees azimuth\n", "* `Slope`: Slope in degrees\n", "* `Horizontal_Distance_To_Hydrology`: Horizontal dist to nearest surface water features in meters\n", "* `Vertical_Distance_To_Hydrology`: Vertical dist to nearest surface water features in meters\n", "* `Horizontal_Distance_To_Roadways`: Horizontal dist to nearest roadway in meters\n", "* `Hillshade_9am`: Hillshade index at 9am, summer solstice\n", "* `Hillshade_Noon`: Hillshade index at noon, summer solstice\n", "* `Hillshade_3pm`: Hillshade index at 3pm, summer solstice\n", "* `Horizontal_Distance_To_Fire_Points`: Horizontal dist to nearest wildfire ignition points, meters\n", "* `Wilderness_Area_x`: Wilderness area designation (4 columns)\n", "* `Soil_Type_x`: Soil Type designation (40 columns)\n", "* `Cover_Type`: 1 for cottonwood/willow, 0 for ponderosa pine"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This is also an imbalanced dataset, since cottonwood/willow trees are relatively rare in this forest:"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Raw Counts\n", "0    35754\n", "1     2747\n", "Name: Cover_Type, dtype: int64\n", "\n", "Percentages\n", "0    0.928651\n", "1    0.071349\n", "Name: Cover_Type, dtype: float64\n"]}], "source": ["print(\"Raw Counts\")\n", "print(df[\"Cover_Type\"].value_counts())\n", "print()\n", "print(\"Percentages\")\n", "print(df[\"Cover_Type\"].value_counts(normalize=True))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If we had a model that *always* said that the cover type was ponderosa pine (class 0), what accuracy score would we get?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "We would get an accuracy score of 0.928651, i.e. about 92.9% accuracy\n", "\n", "This is because about 92.9% of areas are covered by ponderosa pine\n", "\"\"\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["You will need to take this into account when working through this problem."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Requirements\n", "\n", "#### 1. Perform a Train-Test Split\n", "\n", "For a complete end-to-end ML process, we need to create a holdout set that we will use at the very end to evaluate our final model's performance.\n", "\n", "#### 2. Build and Evaluate a Baseline Model\n", "\n", "Without performing any preprocessing or hyperparameter tuning, build and evaluate a vanilla logistic regression model using log loss and `cross_val_score`.\n", "\n", "#### 3. Write a Custom Cross Validation Function\n", "\n", "Because we are using preprocessing techniques that differ for train and validation data, we will need a custom function rather than simply preprocessing the entire `X_train` and using `cross_val_score` from scikit-learn.\n", "\n", "#### 4. Build and Evaluate Additional Logistic Regression Models\n", "\n", "Using the function created in the previous step, build multiple logistic regression models with different hyperparameters in order to minimize log loss.\n", "\n", "#### 5. Choose and Evaluate a Final Model\n", "\n", "Preprocess the full training set and test set appropriately, then evaluate the final model with various classification metrics in addition to log loss."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Perform a Train-Test Split\n", "\n", "This process should be fairly familiar by now. In the cell below, use the variable `df` (that has already been created) in order to create `X` and `y`, then training and test sets using `train_test_split` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)).\n", "\n", "We'll use a random state of 42 and `stratify=y` (to ensure an even balance of fraud/not fraud rows) in the train-test split. Recall that the target is `Cover_Type`."]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["\n", "# Import the relevant function\n", "from sklearn.model_selection import train_test_split\n", "\n", "# Split df into X and y\n", "X = df.drop(\"Cover_Type\", axis=1)\n", "y = df[\"Cover_Type\"]\n", "\n", "# Perform train-test split with random_state=42 and stratify=y\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Check that you have the correct data shape before proceeding:"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["\n", "# X and y training data should have the same number of rows\n", "assert X_train.shape[0] == y_train.shape[0] and X_train.shape[0] == 28875\n", "\n", "# X and y testing data should have the same number of rows\n", "assert X_test.shape[0] == y_test.shape[0] and X_test.shape[0] == 9626\n", "\n", "# Both X should have 33 columns\n", "assert X_train.shape[1] == X_test.shape[1] and X_train.shape[1] == 54\n", "\n", "# Both y should have 1 column\n", "assert len(y_train.shape) == len(y_test.shape) and len(y_train.shape) == 1"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Also, we should have roughly equal percentages of cottonwood/willow trees for train vs. test targets:"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Train percent cottonwood/willow: 0.07134199134199135\n", "Test percent cottonwood/willow:  0.0713692083939331\n"]}], "source": ["print(\"Train percent cottonwood/willow:\", y_train.value_counts(normalize=True)[1])\n", "print(\"Test percent cottonwood/willow: \", y_test.value_counts(normalize=True)[1])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Build and Evaluate a Baseline Model\n", "\n", "Using scikit-learn's `LogisticRegression` model, instantiate a classifier with `random_state=42`. Then use `cross_val_score` with `scoring=\"neg_log_loss\"` to find the average cross-validated log loss for this model on `X_train` and `y_train`.\n", "\n", "* [`LogisticRegression` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n", "* [`cross_val_score` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n", "\n", "(Similar to RMSE, the internal implementation of `cross_val_score` requires that we use \"negative log loss\" instead of just log loss. The code provided negates the result for you.)\n", "\n", "**The code below should produce a warning** but not an error. Because we have not scaled the data, we expect to get a `ConvergenceWarning` five times (once for each fold of cross validation)."]}, {"cell_type": "code", "execution_count": 6, "metadata": {"scrolled": true}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["//anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n", "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n", "\n", "Increase the number of iterations (max_iter) or scale the data as shown in:\n", "    https://scikit-learn.org/stable/modules/preprocessing.html\n", "Please also refer to the documentation for alternative solver options:\n", "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n", "  n_iter_i = _check_optimize_result(\n", "//anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n", "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n", "\n", "Increase the number of iterations (max_iter) or scale the data as shown in:\n", "    https://scikit-learn.org/stable/modules/preprocessing.html\n", "Please also refer to the documentation for alternative solver options:\n", "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n", "  n_iter_i = _check_optimize_result(\n", "//anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n", "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n", "\n", "Increase the number of iterations (max_iter) or scale the data as shown in:\n", "    https://scikit-learn.org/stable/modules/preprocessing.html\n", "Please also refer to the documentation for alternative solver options:\n", "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n", "  n_iter_i = _check_optimize_result(\n", "//anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n", "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n", "\n", "Increase the number of iterations (max_iter) or scale the data as shown in:\n", "    https://scikit-learn.org/stable/modules/preprocessing.html\n", "Please also refer to the documentation for alternative solver options:\n", "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n", "  n_iter_i = _check_optimize_result(\n", "//anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n", "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n", "\n", "Increase the number of iterations (max_iter) or scale the data as shown in:\n", "    https://scikit-learn.org/stable/modules/preprocessing.html\n", "Please also refer to the documentation for alternative solver options:\n", "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n", "  n_iter_i = _check_optimize_result(\n"]}, {"data": {"text/plain": ["0.17116279160065645"]}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": ["\n", "# Import relevant class and function\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.model_selection import cross_val_score\n", "\n", "# Instantiate a LogisticRegression with random_state=42\n", "baseline_model = LogisticRegression(random_state=42)\n", "\n", "# Use cross_val_score with scoring=\"neg_log_loss\" to evaluate the model\n", "# on X_train and y_train\n", "baseline_neg_log_loss_cv = cross_val_score(baseline_model, X_train, y_train, scoring=\"neg_log_loss\")\n", "\n", "baseline_log_loss = -(baseline_neg_log_loss_cv.mean())\n", "baseline_log_loss"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ok, so we are getting the `ConvergenceWarning`s we expected, and log loss of around 0.171 with our baseline model.\n", "\n", "Is that a \"good\" log loss? That's hard to say \u2014 log loss is not particularly interpretable. \n", "\n", "If we had a model that just chose 0 (the majority class) every time, this is the log loss we would get:"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"data": {"text/plain": ["2.4640650865286937"]}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": ["from sklearn.metrics import log_loss\n", "import numpy as np\n", "\n", "log_loss(y_train, np.zeros(len(y_train)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Loss is a metric where lower is better, so our baseline model is clearly an improvement over just guessing the majority class every time.\n", "\n", "Even though it is difficult to interpret, the 0.171 value will be a useful baseline as we continue modeling, to see if we are actually making improvements or just getting slightly better performance by chance.\n", "\n", "We will also use other metrics at the last step in order to describe the final model's performance in a more user-friendly way."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Write a Custom Cross Validation Function\n", "\n", "### Conceptual Considerations\n", "\n", "First, consider: which preprocessing steps should be taken with this dataset? Recall that our data is imbalanced, and that it caused a `ConvergenceWarning` for our baseline model."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "Because of class imbalance, we should add some kind of resampling\n", "step. Specifically we'll use SMOTE.\n", "\n", "We are getting a ConvergenceWarning, which means that the gradient\n", "descent algorithm within the logistic regression is failing to find\n", "an optimized answer. We can also see from looking at the dataset that\n", "some of our variables are quite small (0 or 1) while others range in\n", "the thousands. This indicates that we should add scaling to help\n", "\"flatten\" the landscape being iterated over by normalizing the\n", "various units of the different columns.\n", "\"\"\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["As you likely noted above, we should use some kind of resampling technique to address the large class imbalance. Let's use `SMOTE` (synthetic minority oversampling, [documentation here](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html)), which creates synthetic examples of the minority class to help train the model.\n", "\n", "Does SMOTE work just like a typical scikit-learn transformer, where you fit the transformer on the training data then transform both the training and the test data?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As you also likely noted above, we should use some transformer to normalize the data. Let's use a `StandardScaler` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)).\n", "\n", "Does `StandardScaler` work just like a typical scikit-learn transformer, where you fit the transformer on the training data then transform both the training and the test data?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "Yes, StandardScaler is a scikit-learn transformer so it does work\n", "this way. We transform both the train and test data, using the fit\n", "from the training data only.\n", "\n", "Often we are a bit sloppy and we fit and transform the entire\n", "X_train prior to performing cross validation, meaning we have some\n", "risk of leakage. Ideally every train split within the cross\n", "validation would be fit with its own scaler, something that is \n", "achieved more easily with pipelines.\n", "\"\"\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["(At this point it's a good idea to double-check your answers against the `solution` branch to make sure you understand the setup.)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Using `StratifiedKFold`\n", "\n", "As you can see from the `cross_val_score` documentation linked above, \"under the hood\" it is using `StratifiedKFold` for classification tasks.\n", "\n", "Essentially `StratifiedKFold` is just providing the information you need to make 5 separate train-test splits inside of `X_train`. Then there is other logic within `cross_val_score` to fit and evaluate the provided model.\n", "\n", "So, if our original code looked like this:\n", "\n", "```python\n", "baseline_model = LogisticRegression(random_state=42)\n", "baseline_neg_log_loss_cv = cross_val_score(baseline_model, X_train, y_train, scoring=\"neg_log_loss\")\n", "baseline_log_loss = -(baseline_neg_log_loss_cv.mean())\n", "baseline_log_loss\n", "```\n", "\n", "The equivalent of the above code using `StratifiedKFold` would look something like this:"]}, {"cell_type": "code", "execution_count": 8, "metadata": {"scrolled": true}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["//anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n", "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n", "\n", "Increase the number of iterations (max_iter) or scale the data as shown in:\n", "    https://scikit-learn.org/stable/modules/preprocessing.html\n", "Please also refer to the documentation for alternative solver options:\n", "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n", "  n_iter_i = _check_optimize_result(\n", "//anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n", "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n", "\n", "Increase the number of iterations (max_iter) or scale the data as shown in:\n", "    https://scikit-learn.org/stable/modules/preprocessing.html\n", "Please also refer to the documentation for alternative solver options:\n", "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n", "  n_iter_i = _check_optimize_result(\n", "//anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n", "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n", "\n", "Increase the number of iterations (max_iter) or scale the data as shown in:\n", "    https://scikit-learn.org/stable/modules/preprocessing.html\n", "Please also refer to the documentation for alternative solver options:\n", "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n", "  n_iter_i = _check_optimize_result(\n", "//anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n", "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n", "\n", "Increase the number of iterations (max_iter) or scale the data as shown in:\n", "    https://scikit-learn.org/stable/modules/preprocessing.html\n", "Please also refer to the documentation for alternative solver options:\n", "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n", "  n_iter_i = _check_optimize_result(\n", "//anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n", "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n", "\n", "Increase the number of iterations (max_iter) or scale the data as shown in:\n", "    https://scikit-learn.org/stable/modules/preprocessing.html\n", "Please also refer to the documentation for alternative solver options:\n", "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n", "  n_iter_i = _check_optimize_result(\n"]}, {"data": {"text/plain": ["0.17116279160065645"]}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": ["from sklearn.metrics import make_scorer\n", "from sklearn.model_selection import StratifiedKFold\n", "from sklearn.base import clone\n", "\n", "# Negative log loss doesn't exist as something we can import,\n", "# but we can create it\n", "neg_log_loss = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n", "\n", "# Instantiate the model (same as previous example)\n", "baseline_model = LogisticRegression(random_state=42)\n", "\n", "# Create a list to hold the score from each fold\n", "kfold_scores = np.ndarray(5)\n", "\n", "# Instantiate a splitter object and loop over its result\n", "kfold = StratifiedKFold()\n", "for fold, (train_index, val_index) in enumerate(kfold.split(X_train, y_train)):\n", "    # Extract train and validation subsets using the provided indices\n", "    X_t, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n", "    y_t, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n", "    \n", "    # Clone the provided model and fit it on the train subset\n", "    temp_model = clone(baseline_model)\n", "    temp_model.fit(X_t, y_t)\n", "    \n", "    # Evaluate the provided model on the validation subset\n", "    neg_log_loss_score = neg_log_loss(temp_model, X_val, y_val)\n", "    kfold_scores[fold] = neg_log_loss_score\n", "    \n", "-(kfold_scores.mean())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As you can see, this produced the same result as our original cross validation (including the `ConvergenceWarning`s):"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[-0.17158243 -0.17213642 -0.1629862  -0.17678272 -0.17232618]\n", "[-0.17158243 -0.17213642 -0.1629862  -0.17678272 -0.17232618]\n"]}], "source": ["print(baseline_neg_log_loss_cv)\n", "print(kfold_scores)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["So, what is the point of doing it this way, instead of the much-shorter `cross_val_score` approach?\n", "\n", "**Using `StratifiedKFold` \"by hand\" allows us to customize what happens inside of that loop.**\n", "\n", "Therefore we can apply these preprocessing techniques appropriately:\n", "\n", "1. Fit a `SMOTE` object and transform only the training subset\n", "2. Fit a `StandardScaler` object on the training subset (not the full training data) and transform both the train and test subsets"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Writing a Custom Cross Validation Function with `StratifiedKFold`\n", "\n", "In the cell below, we have set up a function `custom_cross_val_score` that has an interface that resembles the `cross_val_score` function from scikit-learn.\n", "\n", "Most of it is set up for you already, all you need to do is add the `SMOTE` and `StandardScaler` steps described above."]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"data": {"text/plain": ["0.13235904968769785"]}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": ["\n", "# Import relevant sklearn and imblearn classes\n", "from sklearn.preprocessing import StandardScaler\n", "from imblearn.over_sampling import SMOTE\n", "\n", "def custom_cross_val_score(estimator, X, y):\n", "    # Create a list to hold the score from each fold\n", "    kfold_scores = np.ndarray(5)\n", "\n", "    # Instantiate a splitter object and loop over its result\n", "    kfold = StratifiedKFold(n_splits=5)\n", "    for fold, (train_index, val_index) in enumerate(kfold.split(X, y)):\n", "        # Extract train and validation subsets using the provided indices\n", "        X_t, X_val = X.iloc[train_index], X.iloc[val_index]\n", "        y_t, y_val = y.iloc[train_index], y.iloc[val_index]\n", "        \n", "        # Instantiate StandardScaler\n", "        scaler = StandardScaler()\n", "        # Fit and transform X_t\n", "        X_t_scaled = scaler.fit_transform(X_t)\n", "        # Transform X_val\n", "        X_val_scaled = scaler.transform(X_val)\n", "        \n", "        # Instantiate SMOTE with random_state=42 and sampling_strategy=0.28\n", "        sm = SMOTE(random_state=42, sampling_strategy=0.28)\n", "        # Fit and transform X_t_scaled and y_t using sm\n", "        X_t_oversampled, y_t_oversampled = sm.fit_resample(X_t_scaled, y_t)\n", "        \n", "        # Clone the provided model and fit it on the train subset\n", "        temp_model = clone(estimator)\n", "        temp_model.fit(X_t_oversampled, y_t_oversampled)\n", "        \n", "        # Evaluate the provided model on the validation subset\n", "        neg_log_loss_score = neg_log_loss(temp_model, X_val_scaled, y_val)\n", "        kfold_scores[fold] = neg_log_loss_score\n", "        \n", "    return kfold_scores\n", "\n", "model_with_preprocessing = LogisticRegression(random_state=42, class_weight={1: 0.28})\n", "preprocessed_neg_log_loss_cv = custom_cross_val_score(model_with_preprocessing, X_train, y_train)\n", "- (preprocessed_neg_log_loss_cv.mean())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The output you get should be about 0.132, and there should no longer be a `ConvergenceWarning`.\n", "\n", "If you're not getting the right output, double check that you are applying the correct transformations to the correct variables:\n", "\n", "1. `X_t` should be scaled to create `X_t_scaled`, then `X_t_scaled` should be resampled to create `X_t_oversampled`, then `X_t_oversampled` should be used to fit the model\n", "2. `X_val` should be scaled to create `X_val_scaled`, then `X_val_scaled` should be used to evaluate `neg_log_loss`\n", "3. `y_t` should be resampled to create `y_t_oversampled`, then `y_t_oversampled` should be used to fit the model\n", "4. `y_val` should not be transformed in any way. It should just be used to evaluate `neg_log_loss`\n", "\n", "Another thing to check is that you used `sampling_strategy=0.28` when you instantiated the `SMOTE` object."]}, {"cell_type": "markdown", "metadata": {}, "source": ["If you are getting the right output, great!  Let's compare that to our baseline log loss:"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["0.17116279160065645\n", "0.13235904968769785\n"]}], "source": ["print(-baseline_neg_log_loss_cv.mean())\n", "print(-preprocessed_neg_log_loss_cv.mean())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Looks like our preprocessing with `StandardScaler` and `SMOTE` has provided some improvement over the baseline! Let's move on to Step 4."]}], "metadata": {"kernelspec": {"display_name": "Python (learn-env)", "language": "python", "name": "learn-env"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 4}